{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kickstarter Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "from random import randint\n",
    "from IPython.core.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from random import randint\n",
    "from datetime import datetime,date\n",
    "from numpy import nan as Nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DataFrame from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>backers</th>\n",
       "      <th>pledged</th>\n",
       "      <th>goal</th>\n",
       "      <th>pct_funded</th>\n",
       "      <th>successful</th>\n",
       "      <th>funding_start_dt</th>\n",
       "      <th>funding_end_dt</th>\n",
       "      <th>live</th>\n",
       "      <th>location</th>\n",
       "      <th>category</th>\n",
       "      <th>tag</th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fluent Forever, The App: Learn to *Think* in A...</td>\n",
       "      <td>4434</td>\n",
       "      <td>587785.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>2.351140</td>\n",
       "      <td>True</td>\n",
       "      <td>2017-09-19</td>\n",
       "      <td>2017-10-19</td>\n",
       "      <td>False</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Project We Love</td>\n",
       "      <td>Why learn to translate, when you can build flu...</td>\n",
       "      <td>Why learn to translate, when you can learn tot...</td>\n",
       "      <td>https://www.kickstarter.com/projects/gabrielwy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flag・free photo prints - forever!</td>\n",
       "      <td>5120</td>\n",
       "      <td>331949.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>33.194900</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-09-14</td>\n",
       "      <td>2016-10-28</td>\n",
       "      <td>False</td>\n",
       "      <td>Venice, Los Angeles, CA</td>\n",
       "      <td>Apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An app that delivers 20 free photo prints a mo...</td>\n",
       "      <td>Flag is currently available for iOS, you cando...</td>\n",
       "      <td>https://www.kickstarter.com/projects/flag/flag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Devslopes - ANYONE Can Learn to Code</td>\n",
       "      <td>2149</td>\n",
       "      <td>192056.0</td>\n",
       "      <td>39500.0</td>\n",
       "      <td>4.862177</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>2016-05-19</td>\n",
       "      <td>False</td>\n",
       "      <td>Orem, UT</td>\n",
       "      <td>Apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Devslopes is the world's most effective and af...</td>\n",
       "      <td>Devslopes Game Development AcademyLater this y...</td>\n",
       "      <td>https://www.kickstarter.com/projects/912791163...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project backers   pledged  \\\n",
       "0  Fluent Forever, The App: Learn to *Think* in A...    4434  587785.0   \n",
       "1                  Flag・free photo prints - forever!    5120  331949.0   \n",
       "2               Devslopes - ANYONE Can Learn to Code    2149  192056.0   \n",
       "\n",
       "       goal  pct_funded  successful funding_start_dt funding_end_dt   live  \\\n",
       "0  250000.0    2.351140        True       2017-09-19     2017-10-19  False   \n",
       "1   10000.0   33.194900        True       2016-09-14     2016-10-28  False   \n",
       "2   39500.0    4.862177        True       2016-04-19     2016-05-19  False   \n",
       "\n",
       "                  location category              tag  \\\n",
       "0              Chicago, IL     Apps  Project We Love   \n",
       "1  Venice, Los Angeles, CA     Apps              NaN   \n",
       "2                 Orem, UT     Apps              NaN   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Why learn to translate, when you can build flu...   \n",
       "1  An app that delivers 20 free photo prints a mo...   \n",
       "2  Devslopes is the world's most effective and af...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Why learn to translate, when you can learn tot...   \n",
       "1  Flag is currently available for iOS, you cando...   \n",
       "2  Devslopes Game Development AcademyLater this y...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.kickstarter.com/projects/gabrielwy...  \n",
       "1  https://www.kickstarter.com/projects/flag/flag...  \n",
       "2  https://www.kickstarter.com/projects/912791163...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('kickstarter_tech_db',sep='\\t')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "#nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'corpus' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df[['project','summary','description']]:\n",
    "    df[col] = df[col].apply(lambda x: str(x))\n",
    "\n",
    "df['corpus'] = df['project'] + ' ' + df['summary'] + ' ' + df['description']\n",
    "df['corpus'] = df['corpus'].apply(lambda x: x.replace('nan ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also need to clean up messy punctuations (ie when a punctuation is not followed by a space)\n",
    "def pad_punct(c):\n",
    "    if c in string.punctuation:\n",
    "        c = ' '+c+' '\n",
    "    return c\n",
    "\n",
    "df['corpus'] = df['corpus'].apply(lambda x: ''.join([pad_punct(c) for c in str(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import PunktSentenceTokenizer\n",
    "df['corpus_tokenized'] = df['corpus'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Fluent, Forever, ,, The, App, :, Learn, to, *...\n",
       "1    [Flag・free, photo, prints, -, forever, !, An, ...\n",
       "2    [Devslopes, -, ANYONE, Can, Learn, to, Code, D...\n",
       "3    [Dwell, Scripture, Listening, App, Our, Kickst...\n",
       "4    [The, Disaster, Prediction, App, The, Disaster...\n",
       "Name: corpus_tokenized, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corpus_tokenized'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging, Chunking & Chinking of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(tokenized):\n",
    "    \n",
    "    tree,words = [],[]\n",
    "    \n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            \n",
    "            i = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(i)\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+} \n",
    "                                    }<DT|WRB|TO|IN|PRP.*|.|CD|POS|PDT|W.*?|MD|CC|RB.*>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            tree.append(chunked)\n",
    "            \n",
    "        for branch in tree:\n",
    "            for leaf in branch:\n",
    "                if type(leaf) != tuple:\n",
    "                    word_ = str(leaf)\n",
    "                    word_ = re.sub('Chunk ','',word_)\n",
    "                    word_ = re.split('/',word_)[0][1:]\n",
    "                    words.append(word_)\n",
    "        \n",
    "    except Exception as e:\n",
    "        tree.append(str(e))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus_chinked'] = df['corpus_tokenized'].apply(process_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "df['corpus_chinked'] = df['corpus_chinked'].apply(lambda x: [word for word in x if word.lower() not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "df['corpus_chinked'] = df['corpus_chinked'].apply(lambda x: [word for word in x if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fluent', 'App', 'Learn', 'Think', 'Language', 'learn', 'translate', 'build']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corpus_chinked'].iloc[0][:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def multi_lemmatizer(word):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(word)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            return lemma.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            return lemma.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            return lemma.lemmatize(word, pos='a')\n",
    "        else:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus_lemmatized'] = df['corpus_chinked'].apply(lambda x: [multi_lemmatizer(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fluent', 'App', 'Learn', 'Think', 'Language']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corpus_lemmatized'].iloc[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "df['corpus_stemmed'] = df['corpus_lemmatized'].apply(lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove common words found in Kickstarter listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_2 = ['app','apps','software','program','website','get','month','year','add','pledge','beta','shipping','deliver',\n",
    "          'subscription','com','org','kickstarter','offer','online','goal','target','go','make','use','``','iphone',\n",
    "          'android','demand','need','people','person','let','free','support','something','take','application','etc',\n",
    "          'order','version','mobile','funding','detail','launch','product','project','phase','fund','thing','want',\n",
    "          'everything','next','start','us','we','you','new','web','able','fund','funding','funded']\n",
    "\n",
    "df['corpus_lemmatized'] = df['corpus_lemmatized'].apply(lambda x: [word for word in x if word.lower() not in stops_2])\n",
    "df['corpus_lemmatized'] = df['corpus_lemmatized'].apply(lambda x: [word for word in x if len(word)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namedEnt(tokenized):\n",
    "    \n",
    "    tree,words = [],[]\n",
    "    \n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            i = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(i)\n",
    "            namedEnt = nltk.ne_chunk(tagged,binary=True)\n",
    "            tree.append(namedEnt)\n",
    "            \n",
    "        for branch in tree:\n",
    "            for leaf in branch:\n",
    "                if type(leaf) != tuple:\n",
    "                    word_ = str(leaf)\n",
    "                    word_ = re.sub('NE ','',word_)\n",
    "                    word_ = re.split('/',word_)[0][1:]\n",
    "                    words.append(word_)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus_namedEnt'] = df['corpus_lemmatized'].apply(namedEnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove named entities from corpus_lemmatized\n",
    "new_lems = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    a = df['corpus_lemmatized'].iloc[i]\n",
    "    b = df['corpus_namedEnt'].iloc[i]\n",
    "    new_lem = [word for word in a if word not in b]\n",
    "    new_lems.append(new_lem)\n",
    "\n",
    "df['corpus_lemmatized_noNE'] = new_lems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word to word.lower() in corpus_lemmatized\n",
    "df['corpus_lemmatized_noNE'] = df['corpus_lemmatized_noNE'].apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus_top10'] = df['corpus_lemmatized'].apply(lambda x: nltk.FreqDist(x).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(language, 35), (learn, 24), (word, 13), (sen...\n",
       "1    [(print, 26), (Flag, 19), (photo, 16), (ad, 10...\n",
       "2    [(Devslopes, 5), (Game, 3), (Bootcamp, 3), (wo...\n",
       "3    [(listen, 18), (Bible, 15), (audio, 15), (Dwel...\n",
       "4    [(earthquake, 5), (Disaster, 4), (storm, 4), (...\n",
       "Name: corpus_top10, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corpus_top10'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus_top10_clean'] = df['corpus_top10'].apply(lambda x: [word for word,freq in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [language, learn, word, sentence, flashcard, S...\n",
       "1    [print, Flag, photo, ad, send, help, paper, cr...\n",
       "2    [Devslopes, Game, Bootcamp, work, bootcamp, HT...\n",
       "3    [listen, Bible, audio, Dwell, voice, Scripture...\n",
       "4    [earthquake, Disaster, storm, time, solar, dis...\n",
       "Name: corpus_top10_clean, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corpus_top10_clean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document-Term Matrix\n",
    "\n",
    "To run any mathematical model on text corpus, it is a good practice to convert it into a matrix representation. LDA model looks for repeating term patterns in the entire DT matrix.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revector corpora into lists of lists\n",
    "df['corpus_lemmatized_revect'] = df['corpus_lemmatized'].apply(lambda x: [x[i:i+randint(3,5)] \n",
    "                                                                          for i in range(0,len(x),randint(3,5))])\n",
    "\n",
    "# Create the term dictionary of our corpus, where every unique term is assigned an index\n",
    "df['dictionary'] = df['corpus_lemmatized_revect'].apply(lambda x: corpora.Dictionary(x))\n",
    "\n",
    "# Convert list of documents (corpus) into Document-Term Matrix using dictionary prepared above\n",
    "matrices = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    matrix = [df['dictionary'].iloc[i].doc2bow(doc) for doc in df['corpus_lemmatized_revect'].iloc[i]]\n",
    "    matrices.append(matrix)\n",
    "\n",
    "df['doc_term_matrix'] = matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling with Latent Dirichlet Allocation (LDA)\n",
    "Next step is to create an object for LDA model and train it on Document-Term matrix. The training also requires few parameters as input which are explained in the above section. The gensim module allows both LDA model estimation from a training corpus and inference of topic distribution on new, unseen documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object for LDA model using gensim library\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Run & Train LDA model on the document-term matrix\n",
    "ldas = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    #df['dictionary'].iloc[i].filter_extremes(no_below=2, no_above=0.1)\n",
    "    try:\n",
    "        lda_ = lda(df['doc_term_matrix'].iloc[i], num_topics=3, id2word = df['dictionary'].iloc[i], passes=50)\n",
    "    except:\n",
    "        lda_ = None\n",
    "    ldas.append(lda_)\n",
    "\n",
    "df['ldamodel'] = ldas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    LdaModel(num_terms=336, num_topics=3, decay=0....\n",
       "1    LdaModel(num_terms=208, num_topics=3, decay=0....\n",
       "2    LdaModel(num_terms=68, num_topics=3, decay=0.5...\n",
       "3    LdaModel(num_terms=390, num_topics=3, decay=0....\n",
       "4    LdaModel(num_terms=93, num_topics=3, decay=0.5...\n",
       "Name: ldamodel, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ldamodel'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LDA topics to DataFrame\n",
    "topics = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    topics_ = []\n",
    "    try:\n",
    "        for j in df['ldamodel'].iloc[i].print_topics(num_topics=2, num_words=5):\n",
    "            st = j[1]\n",
    "            topics__ = \" \".join(re.findall(\"[a-zA-Z]+\", st))\n",
    "            topics__ = re.sub('nan','',topics__).split()\n",
    "            topics_ += topics__\n",
    "        topics.append(topics_)\n",
    "    except:\n",
    "        topics.append(None)\n",
    "\n",
    "df['ldatopics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list',\n",
       " 'sentence',\n",
       " 'trainer',\n",
       " 'Spanish',\n",
       " 'user',\n",
       " 'language',\n",
       " 'learn',\n",
       " 'review',\n",
       " 'work',\n",
       " 'include']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ldatopics'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAvis\n",
    "https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "\n",
    "pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n",
    "\n",
    "* Saliency: a measure of how much the term tells you about the topic.\n",
    "* Relevance: a weighted average of the probability of the word given the topic and the word given the topic normalized by the probability of the topic.\n",
    "* The size of the bubble measures the importance of the topics, relative to the data.\n",
    "\n",
    "First, we got the most salient terms, means terms mostly tell us about what’s going on relative to the topics. We can also look at individual topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el199891123690311129297688673\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el199891123690311129297688673_data = {\"mdsDat\": {\"Freq\": [38.32289123535156, 31.163497924804688, 30.51361656188965], \"cluster\": [1, 1, 1], \"topics\": [1, 2, 3], \"x\": [0.07986238630726289, 0.02799037084886914, -0.10785275715613196], \"y\": [0.07342837136477028, -0.10146716633699573, 0.02803879497222553]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [13.0, 4.0, 3.0, 3.0, 4.0, 4.0, 7.0, 3.0, 3.0, 3.0, 3.0, 5.0, 18.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 5.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 4.0, 4.2241644859313965, 4.140743732452393, 2.8901617527008057, 2.2231242656707764, 2.2230348587036133, 2.2230048179626465, 2.2229580879211426, 2.2229158878326416, 2.2048568725585938, 1.5559965372085571, 1.5558271408081055, 1.5557281970977783, 1.5557037591934204, 1.5556751489639282, 1.5556644201278687, 1.5556620359420776, 1.5555939674377441, 1.5555888414382935, 1.5553194284439087, 5.620970249176025, 0.8888104557991028, 0.888810396194458, 0.8888052701950073, 0.8888052701950073, 0.8888052701950073, 0.8888052701950073, 0.8887766003608704, 0.8887766003608704, 0.8887766003608704, 0.8887766003608704, 7.106762886047363, 6.8905158042907715, 3.6552016735076904, 3.2070696353912354, 2.975512742996216, 2.914130210876465, 2.9021944999694824, 2.5151021480560303, 2.2207274436950684, 2.213273286819458, 2.2097067832946777, 1.5846173763275146, 1.5756466388702393, 1.5644710063934326, 3.3947408199310303, 3.899848699569702, 2.121433973312378, 2.121405839920044, 2.1213674545288086, 2.119471311569214, 2.1110005378723145, 2.9980483055114746, 1.484789490699768, 1.484777569770813, 1.4847739934921265, 1.4847739934921265, 1.4847291707992554, 1.4846442937850952, 1.4781478643417358, 1.4781473875045776, 0.848279595375061, 0.848279595375061, 0.848279595375061, 0.848279595375061, 0.8482761979103088, 0.8482761979103088, 0.8482761383056641, 0.8482595682144165, 0.848259687423706, 0.848259687423706, 0.8482595682144165, 0.8482332229614258, 0.8482332229614258, 0.8482332229614258, 8.354720115661621, 5.9416022300720215, 2.7601516246795654, 2.456319570541382, 2.1346054077148438, 1.5357270240783691, 1.5201702117919922, 1.500960111618042, 1.4999831914901733, 1.4992824792861938, 1.4946198463439941, 1.4875580072402954, 1.4837782382965088, 1.4774198532104492, 3.2962911128997803, 2.7150936126708984, 2.702554225921631, 2.689563035964966, 2.0865604877471924, 2.067380428314209, 1.4617204666137695, 1.4617204666137695, 1.4615044593811035, 1.461503505706787, 1.461499571800232, 1.4613683223724365, 1.459130883216858, 0.8350974321365356, 0.8350974321365356, 0.8350974321365356, 0.8350974321365356, 0.8350974321365356, 0.8350757360458374, 0.8350757360458374, 0.8350757360458374, 0.8350757360458374, 0.8350695967674255, 0.8350695967674255, 0.8350686430931091, 0.8350685238838196, 0.8350685238838196, 0.8350483179092407, 0.8350370526313782, 0.8350369930267334, 2.777390241622925, 2.713604688644409, 2.647249221801758, 2.1210172176361084, 2.098145008087158, 2.0962305068969727, 2.0307254791259766, 1.4881505966186523, 1.4661802053451538, 1.4398736953735352, 1.1877413988113403, 0.8449254035949707, 0.841593325138092, 0.8410707116127014, 0.8409934043884277, 0.8393905162811279, 0.8380311727523804], \"Term\": [\"learn\", \"review\", \"work\", \"list\", \"choice\", \"additional\", \"word\", \"trainer\", \"user\", \"stretch\", \"include\", \"Spanish\", \"language\", \"full\", \"Korean\", \"Russian\", \"different\", \"date\", \"feature\", \"German\", \"final\", \"level\", \"English\", \"search\", \"option\", \"reward\", \"example\", \"gift\", \"friend\", \"test\", \"choice\", \"additional\", \"final\", \"option\", \"reward\", \"example\", \"gift\", \"friend\", \"Level\", \"check\", \"way\", \"pick\", \"study\", \"select\", \"matter\", \"ready\", \"choose\", \"campaign\", \"provide\", \"word\", \"sweet\", \"stuff\", \"information\", \"term\", \"push\", \"efficient\", \"mean\", \"give\", \"awesome\", \"seem\", \"language\", \"learn\", \"sentence\", \"level\", \"Fluent\", \"image\", \"flashcard\", \"native\", \"create\", \"search\", \"test\", \"access\", \"much\", \"fluency\", \"work\", \"review\", \"Korean\", \"Russian\", \"different\", \"date\", \"German\", \"include\", \"wish\", \"approach\", \"Latin\", \"America\", \"continue\", \"release\", \"Pronunciation\", \"Trainer\", \"length\", \"lifetime\", \"trainerin\", \"Apronunciation\", \"love\", \"Marketing\", \"Thank\", \"browser\", \"capable\", \"dog\", \"HTML5\", \"video\", \"equity\", \"yes\", \"language\", \"learn\", \"Spanish\", \"level\", \"test\", \"follow\", \"backer\", \"Italian\", \"grammar\", \"least\", \"keep\", \"create\", \"Fluent\", \"English\", \"list\", \"trainer\", \"user\", \"stretch\", \"full\", \"feature\", \"Portuguese\", \"Dutch\", \"basic\", \"difference\", \"handy\", \"engine\", \"database\", \"learnYou\", \"soundPlay\", \"PlayReplay\", \"withsound00\", \"content\", \"priority\", \"Last\", \"Day\", \"top\", \"acurated\", \"proofread\", \"Yup\", \"discountof\", \"Alifetime\", \"Castilian\", \"havecomment\", \"submit\", \"sentence\", \"Spanish\", \"language\", \"flashcard\", \"search\", \"English\", \"word\", \"Backers\", \"pronunciation\", \"image\", \"native\", \"couple\", \"come\", \"sort\", \"Stuff\", \"phone\", \"right\"], \"Total\": [13.0, 4.0, 3.0, 3.0, 4.0, 4.0, 7.0, 3.0, 3.0, 3.0, 3.0, 5.0, 18.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 5.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 4.0, 4.6463165283203125, 4.641498565673828, 3.312039852142334, 2.6448986530303955, 2.6448938846588135, 2.6448915004730225, 2.6448898315429688, 2.644887685775757, 2.6440582275390625, 1.9777531623840332, 1.9777448177337646, 1.9777381420135498, 1.977738380432129, 1.977735996246338, 1.9777357578277588, 1.9777354001998901, 1.9777320623397827, 1.9777315855026245, 1.9777170419692993, 7.864572525024414, 1.3106048107147217, 1.3106048107147217, 1.310604453086853, 1.310604453086853, 1.310604453086853, 1.310604453086853, 1.3106032609939575, 1.3106032609939575, 1.3106032609939575, 1.3106032609939575, 18.108732223510742, 13.044325828552246, 7.126216888427734, 5.873011112213135, 5.217141151428223, 4.566946029663086, 6.4655046463012695, 3.916049003601074, 3.9180190563201904, 4.524483680725098, 4.554132461547852, 3.2427878379821777, 2.6152572631835938, 2.6147537231445312, 3.8266007900238037, 4.4611921310424805, 2.553361415863037, 2.553361177444458, 2.553361415863037, 2.5534496307373047, 2.553856372833252, 3.8204517364501953, 1.9167455434799194, 1.916745662689209, 1.9167441129684448, 1.9167441129684448, 1.916745662689209, 1.9167447090148926, 1.9170531034469604, 1.917052984237671, 1.280123233795166, 1.280123233795166, 1.280123233795166, 1.280123233795166, 1.2801235914230347, 1.2801235914230347, 1.2801235914230347, 1.280124306678772, 1.2801244258880615, 1.2801244258880615, 1.280124306678772, 1.2801239490509033, 1.2801239490509033, 1.2801239490509033, 18.108732223510742, 13.044325828552246, 5.696943283081055, 5.873011112213135, 4.554132461547852, 3.2091751098632812, 2.5821709632873535, 2.5437331199645996, 3.226459503173828, 3.250317335128784, 2.543637752532959, 3.9180190563201904, 5.217141151428223, 3.796833038330078, 3.7778728008270264, 3.150423049926758, 3.1506853103637695, 3.151477575302124, 2.523810863494873, 2.5249969959259033, 1.8969676494598389, 1.8969676494598389, 1.896978735923767, 1.8969786167144775, 1.8969788551330566, 1.8969861268997192, 1.8970117568969727, 1.2702419757843018, 1.2702419757843018, 1.2702419757843018, 1.2702419757843018, 1.2702419757843018, 1.2702431678771973, 1.2702431678771973, 1.2702431678771973, 1.2702431678771973, 1.2702436447143555, 1.2702436447143555, 1.2702423334121704, 1.2702423334121704, 1.27024245262146, 1.2702425718307495, 1.2702455520629883, 1.2702455520629883, 7.126216888427734, 5.696943283081055, 18.108732223510742, 6.4655046463012695, 4.524483680725098, 3.796833038330078, 7.864572525024414, 2.5333304405212402, 3.2309558391571045, 4.566946029663086, 3.916049003601074, 1.9367129802703857, 1.9369447231292725, 1.936979055404663, 1.9067953824996948, 1.937088131904602, 1.937180519104004], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8639000058174133, 0.8450000286102295, 0.8228999972343445, 0.7853999733924866, 0.7853999733924866, 0.7853999733924866, 0.7853000164031982, 0.7853000164031982, 0.7774999737739563, 0.7192999720573425, 0.7192000150680542, 0.7190999984741211, 0.7190999984741211, 0.7190999984741211, 0.7190999984741211, 0.7190999984741211, 0.718999981880188, 0.718999981880188, 0.7189000248908997, 0.6233000159263611, 0.5708000063896179, 0.5708000063896179, 0.5708000063896179, 0.5708000063896179, 0.5708000063896179, 0.5708000063896179, 0.5706999897956848, 0.5706999897956848, 0.5706999897956848, 0.5706999897956848, 0.023800000548362732, 0.32089999318122864, 0.2915000021457672, 0.35409998893737793, 0.397599995136261, 0.5098000168800354, 0.15809999406337738, 0.5163999795913696, 0.391400009393692, 0.24410000443458557, 0.23589999973773956, 0.24300000071525574, 0.45239999890327454, 0.445499986410141, 1.0462000370025635, 1.0313999652862549, 0.9805999994277954, 0.9805999994277954, 0.9805999994277954, 0.9796000123023987, 0.9754999876022339, 0.9235000014305115, 0.9106000065803528, 0.9106000065803528, 0.9106000065803528, 0.9106000065803528, 0.9104999899864197, 0.9104999899864197, 0.9059000015258789, 0.9059000015258789, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.7544000148773193, 0.39239999651908875, 0.37950000166893005, 0.44130000472068787, 0.29420000314712524, 0.4081999957561493, 0.42890000343322754, 0.6360999941825867, 0.6384000182151794, 0.4000000059604645, 0.3921999931335449, 0.6341999769210815, 0.19750000536441803, -0.09139999747276306, 0.22210000455379486, 1.0506000518798828, 1.0383000373840332, 1.0335999727249146, 1.028499960899353, 0.9966999888420105, 0.9869999885559082, 0.9264000058174133, 0.9264000058174133, 0.9261999726295471, 0.9261999726295471, 0.9261999726295471, 0.9261000156402588, 0.9246000051498413, 0.7675999999046326, 0.7675999999046326, 0.7675999999046326, 0.7675999999046326, 0.7675999999046326, 0.7675999999046326, 0.7675999999046326, 0.7675999999046326, 0.7675999999046326, 0.7674999833106995, 0.7674999833106995, 0.7674999833106995, 0.7674999833106995, 0.7674999833106995, 0.7674999833106995, 0.7674999833106995, 0.7674999833106995, 0.24469999969005585, 0.44530001282691956, -0.7358999848365784, 0.07240000367164612, 0.41850000619888306, 0.5929999947547913, -0.16699999570846558, 0.6549999713897705, 0.3968999981880188, 0.03269999846816063, -0.006000000052154064, 0.35749998688697815, 0.35339999198913574, 0.35280001163482666, 0.3684000074863434, 0.3506999909877777, 0.3490999937057495], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.013500213623047, -4.0335001945495605, -4.39300012588501, -4.655399799346924, -4.6554999351501465, -4.6554999351501465, -4.6554999351501465, -4.6554999351501465, -4.663700103759766, -5.012199878692627, -5.01230001449585, -5.012400150299072, -5.012400150299072, -5.012400150299072, -5.012400150299072, -5.012400150299072, -5.012499809265137, -5.012499809265137, -5.012700080871582, -3.727799892425537, -5.572199821472168, -5.572199821472168, -5.572199821472168, -5.572199821472168, -5.572199821472168, -5.572199821472168, -5.572199821472168, -5.572199821472168, -5.572199821472168, -5.572199821472168, -3.493299961090088, -3.524199962615967, -4.158199787139893, -4.289000034332275, -4.363900184631348, -4.384799957275391, -4.388899803161621, -4.5320000648498535, -4.656499862670898, -4.659900188446045, -4.661499977111816, -4.99399995803833, -4.99970006942749, -5.006800174713135, -4.025300025939941, -3.8866000175476074, -4.4953999519348145, -4.495500087738037, -4.495500087738037, -4.496399879455566, -4.500400066375732, -4.149600028991699, -4.85230016708374, -4.85230016708374, -4.85230016708374, -4.85230016708374, -4.85230016708374, -4.852399826049805, -4.8566999435424805, -4.8566999435424805, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -5.412099838256836, -3.1247000694274902, -3.46560001373291, -4.2322998046875, -4.348899841308594, -4.489299774169922, -4.81850004196167, -4.828700065612793, -4.841400146484375, -4.842100143432617, -4.84250020980835, -4.845699787139893, -4.850399971008301, -4.85290002822876, -4.8572001457214355, -4.033699989318848, -4.22760009765625, -4.2322998046875, -4.237100124359131, -4.490900039672852, -4.500199794769287, -4.846799850463867, -4.846799850463867, -4.8470001220703125, -4.8470001220703125, -4.8470001220703125, -4.847099781036377, -4.848599910736084, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -5.406700134277344, -4.204999923706055, -4.2281999588012695, -4.252900123596191, -4.474599838256836, -4.485400199890137, -4.486299991607666, -4.518099784851074, -4.82889986038208, -4.843800067901611, -4.8618998527526855, -5.0543999671936035, -5.394999980926514, -5.398900032043457, -5.399499893188477, -5.399600028991699, -5.401500225067139, -5.403200149536133]}, \"token.table\": {\"Topic\": [3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 1, 2, 3, 2, 2, 2, 3, 2, 3, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 1, 2, 3, 3, 1, 2, 1, 1, 2, 3, 2, 1, 2, 1, 1, 1, 1, 3, 3, 2, 1, 3, 1, 2, 3, 2, 3, 2, 3, 2, 1, 3, 2, 1, 3, 1, 1, 2, 3, 1, 2, 1, 2, 3, 1, 3, 1, 1, 1, 2, 3, 3, 3, 1, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 2, 1, 2, 2, 3, 2, 1, 1, 1, 2, 1, 3, 1, 1, 3, 1, 3, 1, 3, 3, 1, 1, 1, 2, 2, 1, 1, 3, 1, 3, 1, 1, 1, 2, 3, 1, 3, 3, 3, 1, 1, 3, 1, 1, 1, 2, 3, 3, 2, 3, 2, 1, 2, 3, 1, 3, 2, 2], \"Freq\": [0.7872512936592102, 0.5217180252075195, 0.7811747789382935, 0.3947373032569885, 0.3947373032569885, 0.7872512340545654, 0.787250816822052, 0.5271571278572083, 0.2633773982524872, 0.5267547965049744, 0.575027585029602, 0.19167585670948029, 0.19167585670948029, 0.7831293940544128, 0.7811741232872009, 0.7862460017204285, 0.39312300086021423, 0.7832812070846558, 0.787250816822052, 0.5217180252075195, 0.7564129829406738, 0.7811746001243591, 0.7872515916824341, 0.5271571278572083, 0.5216339826583862, 0.7832812666893005, 0.5265982151031494, 0.5265982151031494, 0.5244401097297668, 0.5244401097297668, 0.7811746001243591, 0.5216339826583862, 0.787251353263855, 0.6167532801628113, 0.30837664008140564, 0.30837664008140564, 0.7872505187988281, 0.861790657043457, 0.5217176079750061, 0.763007402420044, 0.3872710168361664, 0.7745420336723328, 0.5271540284156799, 0.7811741232872009, 1.0112595558166504, 0.7811740636825562, 1.0112485885620117, 0.8608970046043396, 1.0112593173980713, 0.516277015209198, 0.516277015209198, 0.7872515916824341, 0.5217176079750061, 0.5163387656211853, 0.5163387656211853, 0.5104620456695557, 0.25523102283477783, 0.5271448493003845, 0.7832541465759277, 0.5271540880203247, 0.7832812070846558, 0.787251353263855, 0.7811740636825562, 0.7630066871643066, 0.5271520018577576, 0.78117436170578, 0.7561746835708618, 0.7920801639556885, 0.9057862162590027, 0.4640009105205536, 0.1546669751405716, 0.3093339502811432, 0.7648903727531433, 0.38244518637657166, 0.3116065561771393, 0.6232131123542786, 0.3116065561771393, 0.7561757564544678, 0.7924523949623108, 0.75617516040802, 0.763007402420044, 0.3099372684955597, 0.3099372684955597, 0.3099372684955597, 0.5271540284156799, 0.7872493863105774, 0.6568941473960876, 0.21896471083164215, 0.7852474451065063, 0.26174914836883545, 0.7630066871643066, 0.39313775300979614, 0.39313775300979614, 0.3865538537502289, 0.4417758285999298, 0.16566592454910278, 0.5366318225860596, 0.4599701166152954, 0.7872515916824341, 0.6153245568275452, 0.3076622784137726, 0.7811747789382935, 0.5108112096786499, 0.3405408263206482, 0.7811747789382935, 0.7940976619720459, 0.7811746001243591, 1.0112574100494385, 0.763007402420044, 0.7647431492805481, 0.38237157464027405, 0.7660782337188721, 0.255359411239624, 0.7561726570129395, 0.5162387490272522, 0.5162387490272522, 1.011256217956543, 0.787250816822052, 0.6190118789672852, 0.3095059394836426, 0.7872505187988281, 1.011267066001892, 0.7630066871643066, 1.0112576484680176, 0.52171790599823, 0.8966213464736938, 0.7561740279197693, 0.51621413230896, 0.51621413230896, 0.4420394003391266, 0.4420394003391266, 0.763007402420044, 1.011257290840149, 0.5613076686859131, 0.14032691717147827, 0.4209807217121124, 0.5162678360939026, 0.5162678360939026, 0.7872515916824341, 0.9519344568252563, 1.0112560987472534, 0.7630065083503723, 0.7872493863105774, 0.7630065083503723, 0.7630066871643066, 0.4391615688800812, 0.4391615688800812, 0.787250816822052, 0.9522530436515808, 0.7811747789382935, 0.9521737694740295, 0.78117436170578, 1.011252760887146, 0.5217176675796509, 0.7872515916824341, 0.7629149556159973, 0.254304975271225, 0.7839856147766113, 0.78117436170578], \"Term\": [\"Alifetime\", \"America\", \"Apronunciation\", \"Backers\", \"Backers\", \"Castilian\", \"Day\", \"Dutch\", \"English\", \"English\", \"Fluent\", \"Fluent\", \"Fluent\", \"German\", \"HTML5\", \"Italian\", \"Italian\", \"Korean\", \"Last\", \"Latin\", \"Level\", \"Marketing\", \"PlayReplay\", \"Portuguese\", \"Pronunciation\", \"Russian\", \"Spanish\", \"Spanish\", \"Stuff\", \"Stuff\", \"Thank\", \"Trainer\", \"Yup\", \"access\", \"access\", \"access\", \"acurated\", \"additional\", \"approach\", \"awesome\", \"backer\", \"backer\", \"basic\", \"browser\", \"campaign\", \"capable\", \"check\", \"choice\", \"choose\", \"come\", \"come\", \"content\", \"continue\", \"couple\", \"couple\", \"create\", \"create\", \"database\", \"date\", \"difference\", \"different\", \"discountof\", \"dog\", \"efficient\", \"engine\", \"equity\", \"example\", \"feature\", \"final\", \"flashcard\", \"flashcard\", \"flashcard\", \"fluency\", \"fluency\", \"follow\", \"follow\", \"follow\", \"friend\", \"full\", \"gift\", \"give\", \"grammar\", \"grammar\", \"grammar\", \"handy\", \"havecomment\", \"image\", \"image\", \"include\", \"include\", \"information\", \"keep\", \"keep\", \"language\", \"language\", \"language\", \"learn\", \"learn\", \"learnYou\", \"least\", \"least\", \"length\", \"level\", \"level\", \"lifetime\", \"list\", \"love\", \"matter\", \"mean\", \"much\", \"much\", \"native\", \"native\", \"option\", \"phone\", \"phone\", \"pick\", \"priority\", \"pronunciation\", \"pronunciation\", \"proofread\", \"provide\", \"push\", \"ready\", \"release\", \"review\", \"reward\", \"right\", \"right\", \"search\", \"search\", \"seem\", \"select\", \"sentence\", \"sentence\", \"sentence\", \"sort\", \"sort\", \"soundPlay\", \"stretch\", \"study\", \"stuff\", \"submit\", \"sweet\", \"term\", \"test\", \"test\", \"top\", \"trainer\", \"trainerin\", \"user\", \"video\", \"way\", \"wish\", \"withsound00\", \"word\", \"word\", \"work\", \"yes\"]}, \"R\": 30, \"lambda.step\": 1, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el199891123690311129297688673\", ldavis_el199891123690311129297688673_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el199891123690311129297688673\", ldavis_el199891123690311129297688673_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el199891123690311129297688673\", ldavis_el199891123690311129297688673_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "# First let's test for our first project\n",
    "i = 0\n",
    "\n",
    "lda = df['ldamodel'].iloc[i]\n",
    "docterm = df['doc_term_matrix'].iloc[i]\n",
    "dictionary = df['dictionary'].iloc[i]\n",
    "\n",
    "lda_display  = pyLDAvis.gensim.prepare(lda, docterm, dictionary, sort_topics=True, lambda_step=1)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis_topics = []\n",
    "\n",
    "for i in range(len(df)): \n",
    "    \n",
    "    ldas_ = []\n",
    "    \n",
    "    try:\n",
    "        lda = df['ldamodel'].iloc[i]\n",
    "        corpus = df['doc_term_matrix'].iloc[i]\n",
    "        dictionary = df['dictionary'].iloc[i]\n",
    "        \n",
    "        lda_display  = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "        lda_display_df = lda_display[1]\n",
    "        # Extract top 5 terms per topic (15 total)\n",
    "        topic1 = list(lda_display_df[lda_display_df['Category'] == 'Topic1'].sort_values(['Freq'],ascending=False)['Term'][:5])\n",
    "        topic2 = list(lda_display_df[lda_display_df['Category'] == 'Topic2'].sort_values(['Freq'],ascending=False)['Term'][:5])\n",
    "        topic3 = list(lda_display_df[lda_display_df['Category'] == 'Topic3'].sort_values(['Freq'],ascending=False)['Term'][:5])\n",
    "\n",
    "        ldas_ = topic1 + topic2 + topic3\n",
    "        pyLDAvis_topics.append(ldas_)\n",
    "        \n",
    "    except:\n",
    "        try:\n",
    "            pyLDAvis_topics.append(df['ldatopics'].iloc[i][:5])\n",
    "        except:\n",
    "            pyLDAvis_topics.append(None)\n",
    "\n",
    "df['pyLDAvis_topics'] = pyLDAvis_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [language, learn, word, choice, additional, li...\n",
       "1    [Flag, photo, print, frame, printing, print, p...\n",
       "2    [Devslopes, cover, Unity, Academy, course, Pla...\n",
       "3    [Bible, love, come, verse, playlist, audio, li...\n",
       "4    [ObservatoryProject, Ben, direct, SpaceWeather...\n",
       "Name: pyLDAvis_topics, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pyLDAvis_topics'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll try to reduce our topics to a list of 3 most relevant terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge pyLDAvis and top 10 most frequent terms\n",
    "# later, we'll find the top 3 most recurring terms which will correspond to the intersection of pyLDAvis topics and most frequent terms\n",
    "def topics_transform(x):\n",
    "    try:\n",
    "        return [word.lower() for word in x]\n",
    "    except:\n",
    "        return x\n",
    "    \n",
    "df['pyLDAvis_topics_'] = df['pyLDAvis_topics'] #+ df['corpus_top10_clean']\n",
    "df['pyLDAvis_topics_'] = df['pyLDAvis_topics_'].apply(topics_transform)\n",
    "\n",
    "# Return top 3 topics by frequency/relevance\n",
    "def top3_topics(x):\n",
    "    try:\n",
    "        return nltk.FreqDist(x).most_common(5)\n",
    "    except:\n",
    "        return x\n",
    "    \n",
    "df['pyLDAvis_topics_top3'] = df['pyLDAvis_topics_'].apply(top3_topics)\n",
    "\n",
    "# Clean up format\n",
    "def cleanup_topics(x):\n",
    "    try:\n",
    "        return [word for word,freq in x]\n",
    "    except:\n",
    "        return x\n",
    "df['pyLDAvis_topics_top3_clean'] = df['pyLDAvis_topics_top3'].apply(cleanup_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [language, learn, word, choice, additional]\n",
       "1                   [photo, print, flag, frame, printing]\n",
       "2              [devslopes, cover, unity, academy, course]\n",
       "3                      [bible, listen, love, come, verse]\n",
       "4       [disaster, observatoryproject, ben, direct, sp...\n",
       "5                       [kid, code, cod, show, character]\n",
       "6                 [learn, language, bus, italian, membus]\n",
       "7                 [horse, body, connect, care, equisense]\n",
       "8                   [yoga, level, ultimate, yogi, mental]\n",
       "9            [feature, church, scripture, ministry, voir]\n",
       "10        [service, provider, disabled, help, assistance]\n",
       "11                  [math, prep, prestige, sponsor, test]\n",
       "12                [village, autism, service, ask, friend]\n",
       "13                       [gps, map, outdoor, update, ios]\n",
       "14           [class, developer, time, development, cocoa]\n",
       "15                      [dog, slobbr, pet, boston, owner]\n",
       "16                   [course, developer, time, full, way]\n",
       "17                 [emergency, rapidsos, lee, mrs, phone]\n",
       "18                   [day, second, time, spend, anything]\n",
       "19                    [facets, kids, smart, child, films]\n",
       "20                       [habit, help, change, see, test]\n",
       "21               [keyboard, type, feature, right, hearts]\n",
       "22                     [record, poem, access, data, give]\n",
       "23                      [care, self, friend, garden, day]\n",
       "24              [school, parent, alert, engine, analysis]\n",
       "25                   [child, word, jesus, question, dove]\n",
       "26                        [sparkle, camp, way, idea, eye]\n",
       "27              [pixabay, image, user, august, braxmeier]\n",
       "28              [personals, personal, instagram, ad, non]\n",
       "29              [course, learn, store, courses, complete]\n",
       "                              ...                        \n",
       "7027    [lgbt, community, world, discrimination, commu...\n",
       "7028               [help, provide, require, staff, basis]\n",
       "7029       [accountability, help, time, community, focus]\n",
       "7030      [property, place, student, community, business]\n",
       "7031                     [seller, item, data, time, fba®]\n",
       "7032               [message, thought, pas, friend, share]\n",
       "7033        [professional, member, meeting, cook, advice]\n",
       "7034         [australia, result, rest, world, generation]\n",
       "7035         [country, buyer, anything, browse, location]\n",
       "7036                 [task, complete, lazy, wish, search]\n",
       "7037                    [radio, show, gojc, line, street]\n",
       "7038               [top, kind, category, position, scale]\n",
       "7039          [host, human, interaction, try, technology]\n",
       "7040                    [find, focus, others, leaf, site]\n",
       "7041                        [user, fed, rid, list, quick]\n",
       "7042                  [business, place, work, give, find]\n",
       "7043             [artist, music, best, planning, unknown]\n",
       "7044             [backup, site, server, upgrade, improve]\n",
       "7045        [review, service, deal, informative, surfing]\n",
       "7046                                          [kite, nan]\n",
       "7047                   [keto, resource, lack, save, help]\n",
       "7048                [ebook, page, book, convert, content]\n",
       "7049                 [retail, shop, way, accepted, store]\n",
       "7050                    [life, post, lit, image, feature]\n",
       "7051         [place, creator, developer, gamers, content]\n",
       "7052           [social, ability, profile, medium, answer]\n",
       "7053    [build, tailor, requirementsit, business, system]\n",
       "7054             [cosplay, material, suki, create, sport]\n",
       "7055                 [game, price, video, seller, pocket]\n",
       "7056                 [help, give, work, skilled, provide]\n",
       "Name: pyLDAvis_topics_top3_clean, Length: 7057, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pyLDAvis_topics_top3_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save locally to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('kickstarter_tech_post_nlp_db',sep='\\t',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
